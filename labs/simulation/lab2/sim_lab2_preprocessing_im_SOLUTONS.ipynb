{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health data preprocessing and input modelling exercises.\n",
    "\n",
    "**In this lab you will:**\n",
    "\n",
    "* Gain practical knowledge in pre-processing and analysing real world stochastic health system data\n",
    "* Learn how to fit distribution to data\n",
    "* Learn how to select a suitable distribution for your data\n",
    "\n",
    "> **STUDENT BEWARE**: This lab can be very frustrating! It is designed to show you the sort of data wrangling, analysis and modelling decisions/assumptions you may need to perform in real simulation study.  But do persevere with it (answers are available as well!).  The experience should demonstrate that fitting distributions to real data is difficult and not quite as textbooks make out! By the end of the lab both your `pandas` skills and simulation input modelling skills will have improved.  >_<\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#auto_fit function from pythonhealthcare.org\n",
    "from input_modelling.fitting import auto_fit\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 1**: stroke ward admissions and early supported discharge\n",
    "\n",
    "The dataset we will work with represents admissions and discharges at a hospital acute stroke ward.  \n",
    "\n",
    "The dataset contains the following fields\n",
    "\n",
    "* **patient_type**: stroke, tia (minor stroke), complex-neurological (e.g. brain injury) and other (i.e. non-neurological patients) (str)\n",
    "* **hosp_arrival_date**: date dd-mmm-yy of arrival to hospital (str)\n",
    "* **hosp_arrival_time**: time HH:MM of arrival (str)\n",
    "* **asu_admit_date**: date dd-mmm-yy of admission to the acute stroke unit (str)\n",
    "* **asu_discharge_date**: date dd-mmm-yy of discharge from the acute stroke unit (str)\n",
    "* **wardN_admit_date**: date dd-mmm-yy of admission to the ward N where N is between 2 and 4 (str)\n",
    "* **wardN_discharge_date**: date dd-mmm-yy of discharge from the ward N where N is between 2 and 4 (str)\n",
    "* **hosp_discharge_date**: date dd-mmm-yy of discharge from hospital (str)\n",
    "* **hosp_discharge_time**: time HH:MM of discharge from hospital (str)\n",
    "* **esd**: Early supported discharge binary 0/1. 1 == patient underwent ESD\n",
    "\n",
    "**In this exercise you will need to:**\n",
    "* Make some assumptions about how to model LoS.\n",
    "* Preprocess and wrangle the data set\n",
    "* Perform some exploratory analysis\n",
    "* Fit one or more distributions to decide how to model the arrival process\n",
    "* Fit one or more distributions to decide how to model length stay on the ward.\n",
    "\n",
    "### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acute_strokes = pd.read_csv('../../../../input_modelling_data/input_data_asu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acute_strokes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at the head of the dataset\n",
    "acute_strokes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#any missing data?\n",
    "acute_strokes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: PRE-PROCESSING THE DATA\n",
    "\n",
    "## **Exercise 1.1**: reformat the date fields\n",
    "\n",
    "The date fields as-is are not suitable for analysis in python. Before we can do any analysis we need to pre-process the dates.\n",
    "\n",
    "**Task**:\n",
    "\n",
    "* Convert `acute_admit` and `asu_discharge` into valid date fields: format = YYYY-MM-DD\n",
    "\n",
    "**Hints**\n",
    "\n",
    "* Take a look at the `pd.to_datetime()` function\n",
    "* The dates are in UK format.  You will need to look at what options `pandas` provides to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example answer\n",
    "\n",
    "date_fields = ['hosp_arrival_date', \n",
    "               'asu_admit_date',\n",
    "               'asu_discharge_date',\n",
    "               'hosp_discharge_date']\n",
    "\n",
    "for field in date_fields:\n",
    "    acute_strokes[field] = pd.to_datetime(acute_strokes[field], dayfirst=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acute_strokes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acute_strokes['hosp_arrival_dt'] = pd.to_datetime(acute_strokes['hosp_arrival_date'].astype('str') + ' ' + acute_strokes['hosp_arrival_time'])\n",
    "acute_strokes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acute_strokes['hosp_discharge_dt'] = pd.to_datetime(acute_strokes['hosp_discharge_date'].astype('str') + ' ' + acute_strokes['hosp_discharge_time'])\n",
    "acute_strokes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acute_strokes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 1.2** Sort the admissions by date\n",
    "\n",
    "**Task**:\n",
    "* Put the data into ascending order sorted by`asu_admit` \n",
    "\n",
    "**Hints**:\n",
    "* Take a look at `pd.DataFrame.sort_values(by)`\n",
    "* Make sure you take a look at what `sort_values` does and what it returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example answer\n",
    "acute_strokes = acute_strokes.sort_values(by='hosp_arrival_dt')\n",
    "acute_strokes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 1.3** Recoding the `patient_type` field\n",
    "\n",
    "It is not strictly necessary, but its often useful to get recode fields of type **str** to an **int**\n",
    "\n",
    "Let's take a look at the unique codings we have for patient_type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acute_strokes.patient_type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have four types of basic classification.  Let's recode them as follows:\n",
    "* Stroke: 0\n",
    "* Complex-neurological: 1\n",
    "* Other: 2\n",
    "* TIA: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "* Either recode or create a new field that encodes the type of patient as above. The output of your preprocessing will be a numeric field.\n",
    "* Confirm your recoding has worked (you can always reload the data an preprocess again if needed!)\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "* There are multiple ways to complete this exercise.  One option is to make use of `pd.DataFrame.replace()` method.  For that you will need to create a dict that maps a str to an int e.g.\n",
    "\n",
    "```python\n",
    "example_mapping = {'foo':0,\n",
    "                   'bar':1,\n",
    "                   'spam':2}\n",
    "\n",
    "example_df.replace(example_mapping)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example answer ..\n",
    "#we will store the recoding in a \n",
    "recoded_patient_type = {'Stroke':0, \n",
    "                        'Complex-neurological':1, \n",
    "                        'TIA':2, \n",
    "                        'Other':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acute_strokes = acute_strokes.replace(recoded_patient_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acute_strokes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.4: Drop the redundant fields \n",
    "\n",
    "**Task:**\n",
    "* Drop the redundant fields (provided in a list below) from the dataframe.\n",
    "* Save the to cleaned data a new CSV file `clean_arrival_data_student.csv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['hosp_arrival_date', 'hosp_arrival_time',\n",
    "       'asu_admit_date', 'asu_discharge_date', 'ward2_admit_date',\n",
    "       'ward2_discharge_date', 'ward3_admit_date', 'ward3_discharge_date',\n",
    "       'ward4_admit_date', 'ward4_discharge_date', 'hosp_discharge_date',\n",
    "       'hosp_discharge_time']\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['hosp_arrival_date', 'hosp_arrival_time',\n",
    "       'asu_admit_date', 'asu_discharge_date', 'ward2_admit_date',\n",
    "       'ward2_discharge_date', 'ward3_admit_date', 'ward3_discharge_date',\n",
    "       'ward4_admit_date', 'ward4_discharge_date', 'hosp_discharge_date',\n",
    "       'hosp_discharge_time']\n",
    "\n",
    "acute_strokes = acute_strokes.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acute_strokes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acute_strokes.to_csv('cleaned_arrival_data_student.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: INTER-ARRIVAL TIMES\n",
    "\n",
    "## **Exercise 2.1** Explore the interarrival distributions\n",
    "\n",
    "A first task in input modelling is often to explore the interarrival distributions.  This is often a major source of variability in stochastic health systems.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "* Using your **pre-sorted** data calculate the days between arrivals.  We will ignore patient types for the moment.\n",
    "* Plot a histogram of interarrival times\n",
    "* Calculate the mean and stdev of the interarrival times\n",
    "* What might be a reasonable distribution?\n",
    "* What does the histogram suggest might be going on in the data?\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "* Checkout the `pd.DataFrame.diff()` method\n",
    "* This will return `timedelta64`.  To convert to hours you need to divide by:\n",
    "\n",
    "```python\n",
    "np.timedelta64(24, 'h')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to load cleaned dataset.\n",
    "acute_strokes = pd.read_csv('precleaned_arrival_data.csv', \n",
    "                             parse_dates=['hosp_arrival_dt', \n",
    "                                          'asu_admit_dt', \n",
    "                                          'hosp_discharge_dt'])\n",
    "acute_strokes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example answer ...\n",
    "inter_arrivals = acute_strokes['hosp_arrival_dt'].diff() / np.timedelta64(24, 'h')\n",
    "inter_arrivals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_arrivals.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_arrivals.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_arrivals.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 2.2** Subgroup interarrival distributions.\n",
    "\n",
    "We know that there are at least four categories of patients that use the system.  The histogram of the overall population suggests that we should explore if the distributions vary by group.\n",
    "\n",
    "**Task:**\n",
    "* For each sub group (stroke, complex, tia and other)\n",
    "    * summarise the subgroup sample size\n",
    "    * calculate the subgroups interarrival days (i.e. the days between consecutive stroke arrivals)\n",
    "    * plot the distribution as a histogram\n",
    "    * calculate the mean and standard dev\n",
    "    \n",
    "* Without doing a formal statistical test, what distributions do you think would be useful?\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "* You can plot each of histograms seperately.  But if you wanted to plot histograms in a grid remember `Matplotlib` has `fig, ax = plot.subplots(nrows, ncols)` where `ax` is a numpy matrix and each element represents the corresponding axis of a chart in the grid.\n",
    "* When plotting histograms you could try different numbers of bins.  A good starting number is 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example answer ...\n",
    "iat_by_group = {}\n",
    "\n",
    "for patient_type in range(4):\n",
    "    #calculate individual iat\n",
    "    data = acute_strokes.loc[acute_strokes['patient_type'] \\\n",
    "            == patient_type]['hosp_arrival_dt'].diff() / np.timedelta64(24, 'h')\n",
    "\n",
    "    #print mean\n",
    "    print(f'{patient_type}: n:{data.shape[0]}, m:{data.dropna().mean():.2f}, s:{data.dropna().std():.2f}')\n",
    "    \n",
    "    #store by group\n",
    "    iat_by_group[f'{patient_type}'] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histograms on grid sharing the same x-axis\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 5), sharex=True)\n",
    "\n",
    "key = 0\n",
    "for row in range(2):\n",
    "    for col in range(2):\n",
    "        #plot histogram dropping initial NAN to avoid matplotlib warning/moaning\n",
    "        ax[row][col].hist(iat_by_group[f'{key}'].dropna(), bins=20, \n",
    "                          density=True)\n",
    "        key += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you wanted to store subgroup means\n",
    "means = [items.dropna().to_numpy().mean() for group, items in iat_by_group.items()]\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and subgroup std\n",
    "stds = [items.dropna().to_numpy().std() for group, items in iat_by_group.items()]\n",
    "stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 2.3** Interarrivals: Stroke with ESD versus Stroke with no-ESD\n",
    "\n",
    "Let's explore if Early Supported Discharge patients have a different arrival process to non-ESD patients\n",
    "\n",
    "**Task:**\n",
    "\n",
    "* Using the field 'esd' split the **stroke** patients into two groups:  ESD stroke patients and non-ESD patients.\n",
    "* Plot the distributions\n",
    "* Calculate the mean and standard deviation of each subgroup.\n",
    "* You should find that the mean and stdev are roughly the same for each group.  \n",
    "    * What distribution does this suggest is a good choice and what is special about this distribution that means you don't need to model it as two seperate arrival processes?\n",
    "    * What percentage of stroke arrivals go on to have ESD?  How might you model that?\n",
    "* Using `auto_fit()` test if the exponential distribution is rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_arrivals_esd = acute_strokes.loc[(acute_strokes['esd'] == 1) & (acute_strokes['patient_type'] == 0)]['hosp_arrival_dt'].diff() / np.timedelta64(1, 'h')\n",
    "inter_arrivals_esd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_arrivals_no_esd = acute_strokes.loc[(acute_strokes['esd'] == 0) & (acute_strokes['patient_type'] == 0)]['hosp_arrival_dt'].diff() / np.timedelta64(1, 'h')\n",
    "inter_arrivals_no_esd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prop of arrivals that are ESD\n",
    "inter_arrivals_esd.shape[0] / (inter_arrivals_esd.shape[0] + inter_arrivals_no_esd.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histograms on grid\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 5), sharey=True, sharex=True)\n",
    "\n",
    "ax[0].hist(inter_arrivals_no_esd.dropna(), density=True);\n",
    "ax[1].hist(inter_arrivals_esd.dropna(), density=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_arrivals_esd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_arrivals_no_esd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doesn't reject exponential/\n",
    "auto_fit(inter_arrivals_no_esd, hist=True, pp=True, dist_names=['expon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**possible answer**\n",
    "\n",
    "The mean and the std of the ESD and non-ESD arrivals are for practical purposes the equal. Auto-fit suggests does not rule out an exponential distribution (i.e. a poission process).  Therefore an option would be to model both of these populations as a single arrival process where on arrival patients are labelled either 1 for ESD or for 0 ESD following a Bernoulli distribution. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3: LENGTH OF STAY\n",
    "\n",
    "To estimate distributions of length of stay we need to calculate the difference in days between admission and discharge.\n",
    "\n",
    "**Before we do this there are a few complications in the dataset that we will need to manage as best we can**. The wrangled data is provided in `asu_discharges.csv`\n",
    "\n",
    "Issues with current data:\n",
    "* The data only contain a date stamp for admission to the ASU (there is no time), but we do have a hospital arrival timestamp. These can be on the same day or several (or many) days apart.\n",
    "* The data include a hospital discharge datetime stamp BUT this does not always represent a discharge from ASU.\n",
    "* This is because some of the patients in the data are transfered from the ASU to multiple hospital wards before they are discharged.  \n",
    "    \n",
    ">To handle these issues we will make some assumptions.  We'll assume that the sample of patients that only stayed on the asu is representative of the whole population and also make use of a target for getting admitting patients onto a stroke unit within 4 hours of arrival.\n",
    "\n",
    "This is what we will do:\n",
    "\n",
    "* We will limit our analysis of LoS to patients who only stay on a **single** ward. \n",
    "* If patients are admitted to the ASU on a different day from arrival to hospital we will assume admission time is midnight.\n",
    "* If patients are admitted to the ASU on the same day as arrival to hospital we will assume patients are admitted within 4 hours of their hospital arrival unless that crosses midnight and then we will set it to 23:59.\n",
    "\n",
    "**Note: In practice we would analyse these implications of these decisions during modelling through a process called *sensitivity analysis*.**\n",
    "\n",
    "## **Exercise 3.1**: LoS by `patient_type`\n",
    "\n",
    "**Task:**\n",
    "\n",
    "* Load `asu_discharges.csv` (code provided below)\n",
    "* Create a new field called `los` that is equal the patients length of stay in the asu **in days.**\n",
    "* Explore the distribution of LoS by `patient_type` subgroup.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "* In `pandas` when you calculate the difference between two `datetime64` values you are returned a `timedelta64`.  To convert this into days you need to divide by `np.timedelta64(24, 'h')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "asu_discharges = pd.read_csv('../../../../input_modelling_data/asu_discharges.csv', \n",
    "                             parse_dates=['hosp_arrival_dt', \n",
    "                                          'asu_admit_dt', \n",
    "                                          'hosp_discharge_dt'])\n",
    "asu_discharges.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here to calculate the difference in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example answer\n",
    "asu_discharges['los'] = (asu_discharges['hosp_discharge_dt'] - \\\n",
    "                    asu_discharges['asu_admit_dt']) / np.timedelta64(24, 'h')\n",
    "\n",
    "asu_discharges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through and create LoS by subgroup\n",
    "los_by_group = {}\n",
    "\n",
    "for patient_type in range(4):\n",
    "    #calculate individual los\n",
    "    data = asu_discharges.loc[asu_discharges['patient_type'] \\\n",
    "            == patient_type]['los']\n",
    "\n",
    "    #print mean\n",
    "    print(f'{patient_type}: n:{data.shape[0]}, m:{data.dropna().mean():.2f}, s:{data.dropna().std():.2f}')\n",
    "    \n",
    "    #store by group\n",
    "    los_by_group[f'{patient_type}'] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histograms on grid\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 5), sharex=True)\n",
    "\n",
    "key = 0\n",
    "for row in range(2):\n",
    "    for col in range(2):\n",
    "        #plot histogram dropping initial NAN to avoid matplotlib warning/moaning\n",
    "        ax[row][col].hist(los_by_group[f'{key}'].dropna(), bins=50)\n",
    "        key += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto fit\n",
    "\n",
    "key = 0\n",
    "for key in range(4):\n",
    "    auto_fit(los_by_group[f'{key}'].dropna(), hist=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:** There is no perfect distribution here.  But lognormal seems to be the best fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2. LoS ESD versus Non-ESD\n",
    "\n",
    "**Task**:\n",
    "* Using the `esd` and `patient_type` fields split the dataset into strokes with ESD and strokes without ESD.  \n",
    "* Generate summary statistics for each group.\n",
    "* Use auto-fit \n",
    "* Which distributions do you think are a good choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Los distributions for esd\n",
    "los_stroke_esd = asu_discharges.loc[(asu_discharges['esd'] == 1) \\\n",
    "                                & (asu_discharges['patient_type'] == 0)]['los']\n",
    "\n",
    "los_stroke_no_esd = asu_discharges.loc[(asu_discharges['esd'] == 0) \\\n",
    "                                & (asu_discharges['patient_type'] == 0)]['los']\n",
    "\n",
    "print('ESD LoS summary')\n",
    "print(los_stroke_esd.describe())\n",
    "print('\\n------------------\\nNO ESD LOS summary')\n",
    "print(los_stroke_no_esd.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histograms on grid\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 5), sharey=True, sharex=True)\n",
    "\n",
    "ax[0].hist(los_stroke_esd.dropna(), density=True, bins=40);\n",
    "ax[1].hist(los_stroke_no_esd.dropna(), density=True, bins=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_names = ['beta',\n",
    "              'expon',\n",
    "              'lognorm',\n",
    "              'norm',\n",
    "              'pearson3',\n",
    "              'weibull_min', \n",
    "              'weibull_max']\n",
    "\n",
    "auto_fit(los_stroke_no_esd, hist=True, pp=False, dist_names=dist_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_fit(los_stroke_esd, hist=True, pp=False, dist_names=dist_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions. ESD versus no ESD los.  \n",
    "\n",
    "No distribution is a perfect fit due to high kertosis, but lognormal is reasonable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END OF LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
